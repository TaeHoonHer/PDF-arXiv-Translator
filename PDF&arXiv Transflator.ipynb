{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import PyPDF2\n",
    "import anthropic\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_arxiv_id(url: str) -> str:\n",
    "    return url.split('/')[-1] if 'arxiv.org' in url else url\n",
    "\n",
    "def remove_latex_commands(text: str) -> str:\n",
    "    text = re.sub(r'\\\\begin{CJK\\*}\\{.*?\\}\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\\\end{CJK\\*}', '', text)\n",
    "    return text\n",
    "\n",
    "def translate_text(text: str, paper_info: dict, chunk_size: int, target_language: str = \"Korean\") -> str:\n",
    "    cleaned_text = remove_latex_commands(text)\n",
    "    logging.debug(\"Sending translation request to Claude API.\")\n",
    "\n",
    "    retry_attempts = 3\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            client = anthropic.Client(os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "            response = client.completion(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                max_tokens_to_sample=3000,\n",
    "                prompt=f\"\"\"\n",
    "                LaTeX 구조와 형식을 유지하면서 {target_language}로 번역\n",
    "                \n",
    "                번역:\n",
    "                {cleaned_text}\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            translated_content = response['completion']\n",
    "            translation_result = json.loads(translated_content)\n",
    "            translation_lines = translation_result[\"translate\"]['lines']\n",
    "            translated_line_count = len(translation_lines)\n",
    "\n",
    "            if translated_line_count != chunk_size:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "\n",
    "            return ''.join(translation_lines)\n",
    "\n",
    "        except Exception as error:\n",
    "            logging.error(f\"Error during translation attempt {attempt + 1}: {error}\")\n",
    "            if attempt == retry_attempts - 1:\n",
    "                raise\n",
    "\n",
    "    raise Exception(\"Translation failed after multiple attempts.\")\n",
    "\n",
    "def add_custom_font_to_tex(tex_file_path: str, font_name: str = \"Noto Sans KR\", mono_font_name: str = \"Noto Sans KR\"):\n",
    "    remove_cjk_related_lines(tex_file_path)\n",
    "    font_setup = rf\"\"\"\n",
    "        \\usepackage{{kotex}}\n",
    "        \\usepackage{{xeCJK}}\n",
    "        \\setCJKmainfont{{{font_name}}}\n",
    "        \\setCJKmonofont{{{mono_font_name}}}\n",
    "        \\xeCJKsetup{{CJKspace=true}}\n",
    "        \"\"\"\n",
    "    try:\n",
    "        with open(tex_file_path, 'r+', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith(r'\\documentclass'):\n",
    "                    lines.insert(i + 1, font_setup)\n",
    "                    break\n",
    "            file.seek(0)\n",
    "            file.writelines(lines)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to add custom font: {e}\")\n",
    "        raise\n",
    "\n",
    "def remove_cjk_related_lines(tex_file_path: str):\n",
    "    cjk_related_keywords = [\n",
    "        r'\\usepackage{CJKutf8}',\n",
    "        r'\\usepackage{kotex}',\n",
    "        r'\\begin{CJK}',\n",
    "        r'\\end{CJK}',\n",
    "        r'\\CJKfamily',\n",
    "        r'\\CJK@',\n",
    "        r'\\CJKrmdefault',\n",
    "        r'\\CJKsfdefault',\n",
    "        r'\\CJKttdefault',\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with open(tex_file_path, 'r+', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            new_lines = [line for line in lines if not any(keyword in line for keyword in cjk_related_keywords)]\n",
    "            file.seek(0)\n",
    "            file.writelines(new_lines)\n",
    "            file.truncate()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to remove CJK related lines: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_and_translate_tex_files(directory: str, paper_info: dict, read_lines: int = 30,\n",
    "                                    target_language: str = \"Korean\", max_parallel_tasks: int = 8):\n",
    "    file_line_chunks = []\n",
    "    total_chunks = 0\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tex\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                original_file_path = file_path + \"_original\"\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                    with open(original_file_path, 'w', encoding='utf-8') as original_f:\n",
    "                        original_f.writelines(lines)\n",
    "\n",
    "                    chunks = chunk_lines_safely(lines, read_lines)\n",
    "\n",
    "                    for idx, chunk in enumerate(chunks):\n",
    "                        file_line_chunks.append((file_path, idx, chunk))\n",
    "                    total_chunks += len(chunks)\n",
    "\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.writelines(lines)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error reading or writing file {file_path}: {e}\")\n",
    "\n",
    "    if total_chunks == 0:\n",
    "        logging.warning(\"No lines to translate.\")\n",
    "        return\n",
    "\n",
    "    completed_chunks = 0\n",
    "\n",
    "    def translate_chunk(file_chunk_info):\n",
    "        nonlocal completed_chunks\n",
    "        file_path, chunk_idx, chunk = file_chunk_info\n",
    "        try:\n",
    "            formatted_chunk = [line for idx, line in enumerate(chunk)]\n",
    "            translated_text = translate_text(json.dumps(formatted_chunk), paper_info, len(chunk), target_language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error translating chunk in file {file_path}: {e}\")\n",
    "            translated_text = ''.join(chunk)\n",
    "\n",
    "        completed_chunks += 1\n",
    "        progress = (completed_chunks / total_chunks) * 100\n",
    "        logging.info(f\"Translation progress: {progress:.2f}% completed.\")\n",
    "        return (file_path, chunk_idx, translated_text)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallel_tasks) as executor:\n",
    "        translated_pairs = list(executor.map(translate_chunk, file_line_chunks))\n",
    "\n",
    "    file_contents = {}\n",
    "    for file_path, chunk_idx, translated_chunk in translated_pairs:\n",
    "        if file_path not in file_contents:\n",
    "            file_contents[file_path] = []\n",
    "        file_contents[file_path].append((chunk_idx, translated_chunk))\n",
    "\n",
    "    for file_path, chunks in file_contents.items():\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x[0])\n",
    "        translated_content = ''.join(chunk for _, chunk in sorted_chunks)\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(translated_content)\n",
    "            logging.info(f\"File translated and saved: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing translated content to {file_path}: {e}\")\n",
    "\n",
    "def chunk_lines_safely(lines, lines_per_chunk):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_line_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        current_chunk.append(line)\n",
    "        current_line_count += 1\n",
    "\n",
    "        if current_line_count >= lines_per_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_line_count = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def extract_tar_gz(tar_file_path: str, extract_to: str):\n",
    "    try:\n",
    "        with tarfile.open(tar_file_path, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(path=extract_to)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract tar.gz file: {e}\")\n",
    "        raise\n",
    "\n",
    "def find_main_tex_file(directory: str) -> str:\n",
    "    candidate_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tex\") and \"_original\" not in file:\n",
    "                candidate_files.append(os.path.join(root, file))\n",
    "\n",
    "    main_candidates = []\n",
    "    for file in candidate_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                contents = f.read()\n",
    "                if r'\\documentclass' in contents:\n",
    "                    if any(keyword in contents for keyword in [r'\\begin{document}', r'\\usepackage', r'\\title', r'\\author']):\n",
    "                        main_candidates.append(file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to read file {file}: {e}\")\n",
    "\n",
    "    if main_candidates:\n",
    "        return main_candidates[0]\n",
    "\n",
    "    if candidate_files:\n",
    "        return max(candidate_files, key=os.path.getsize, default=None)\n",
    "\n",
    "    logging.warning(\"No .tex files found.\")\n",
    "    return None\n",
    "\n",
    "def compile_main_tex(directory: str, arxiv_id: str, font_name: str = \"Noto Sans KR\"):\n",
    "    main_tex_path = find_main_tex_file(directory)\n",
    "    if main_tex_path:\n",
    "        add_custom_font_to_tex(main_tex_path, font_name)\n",
    "        compile_tex_to_pdf(main_tex_path, arxiv_id, compile_twice=True)\n",
    "    else:\n",
    "        logging.error(\"Main .tex file not found. Compilation aborted.\")\n",
    "\n",
    "def compile_tex_to_pdf(tex_file_path: str, arxiv_id: str, compile_twice: bool = True):\n",
    "    tex_dir = os.path.dirname(tex_file_path)\n",
    "    tex_file = os.path.basename(tex_file_path)\n",
    "\n",
    "    try:\n",
    "        for _ in range(2 if compile_twice else 1):\n",
    "            result = subprocess.run(\n",
    "                ['xelatex', '-interaction=nonstopmode', tex_file],\n",
    "                cwd=tex_dir,\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            logging.info(f\"xelatex output: {result.stdout}\")\n",
    "            logging.info(f\"xelatex errors: {result.stderr}\")\n",
    "\n",
    "        output_pdf = os.path.join(tex_dir, tex_file.replace(\".tex\", \".pdf\"))\n",
    "        if os.path.exists(output_pdf):\n",
    "            current_dir = os.getcwd()\n",
    "            final_pdf_path = os.path.join(current_dir, f\"{arxiv_id}.pdf\")\n",
    "            os.rename(output_pdf, final_pdf_path)\n",
    "            logging.info(f\"PDF compiled and saved as: {final_pdf_path}\")\n",
    "        else:\n",
    "            logging.error(\"PDF output not found after compilation.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compile TeX file: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_arxiv_intro_and_tex(arxiv_id: str, download_dir: str, target_language: str = \"Korean\",\n",
    "                                 font_name: str = \"Noto Sans KR\"):\n",
    "    arxiv_url = f\"https://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(arxiv_url)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch arXiv metadata: {e}\")\n",
    "        raise\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "    entry = soup.find('entry')\n",
    "    if not entry:\n",
    "        logging.error(\"ArXiv entry not found\")\n",
    "        raise ValueError(\"ArXiv entry not found\")\n",
    "\n",
    "    paper_info = {\n",
    "        \"title\": entry.find('title').text,\n",
    "        \"abstract\": entry.find('summary').text\n",
    "    }\n",
    "\n",
    "    tar_url = f\"https://arxiv.org/src/{arxiv_id}\"\n",
    "    tar_file_path = os.path.join(download_dir, f\"{arxiv_id}.tar.gz\")\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with requests.get(tar_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(tar_file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"arXiv 다운 실패: {e}\")\n",
    "        raise\n",
    "\n",
    "    extract_to = os.path.join(download_dir, arxiv_id)\n",
    "\n",
    "    if os.path.exists(extract_to):\n",
    "        shutil.rmtree(extract_to)\n",
    "\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    extract_tar_gz(tar_file_path, extract_to)\n",
    "    process_and_translate_tex_files(extract_to, paper_info, target_language=target_language)\n",
    "    compile_main_tex(extract_to, arxiv_id, font_name)\n",
    "\n",
    "def translate_pdf(pdf_path: str, target_language: str = \"Korean\"):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            translated_text = \"\"\n",
    "\n",
    "            for page_num in range(num_pages):\n",
    "                page = reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                translated_chunk = translate_text(text, {}, len(text.split('\\n')), target_language)\n",
    "                translated_text += translated_chunk\n",
    "\n",
    "            output_path = pdf_path.replace('.pdf', f'_translated_{target_language}.txt')\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(translated_text)\n",
    "\n",
    "            logging.info(f\"Translated PDF saved as: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error translating PDF: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arxiv_input = input(\"Enter ArXiv ID, URL, or local PDF path: \")\n",
    "    if arxiv_input.endswith('.pdf'):\n",
    "        translate_pdf(arxiv_input)\n",
    "    else:\n",
    "        arxiv_id = extract_arxiv_id(arxiv_input)\n",
    "        download_dir = 'arxiv_downloads'\n",
    "        download_arxiv_intro_and_tex(arxiv_id, download_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
